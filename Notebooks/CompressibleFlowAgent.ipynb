{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f264be",
   "metadata": {},
   "source": [
    "# Compressible Flow Agent\n",
    "The purpose of this notebook is the development of an LLM based agent that is capable of responding to questions about the compressible flow relations and altitude properties that are useful in characerizing the performance of an aircraft.\n",
    "\n",
    "This capability is being developed as an initial attempt to develop LLM-powered agents that are capable of interfacing with physics-based models for design purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c89da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from matplotlib import colors, ticker\n",
    "\n",
    "import requests\n",
    "\n",
    "from ambiance import Atmosphere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee42ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM interface\n",
    "class OllamaLLM:\n",
    "  def __init__(self, model=\"gpt-oss:20b\", host=\"http://localhost:11434\"):\n",
    "    self.model = model\n",
    "    self.host = host\n",
    "\n",
    "  def complete(self, prompt: str) -> str:\n",
    "    response = requests.post(\n",
    "      f\"{self.host}/api/generate\",\n",
    "      json={\"model\": self.model, \"prompt\": prompt, \"stream\": False}\n",
    "  )\n",
    "    return response.json()[\"response\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca33dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You’re chatting with a model based on OpenAI’s **GPT‑4** architecture – the same core technology that powers ChatGPT.  In short, it’s the GPT‑4 language model, fine‑tuned and deployed as the conversational assistant you’re using right now.\n"
     ]
    }
   ],
   "source": [
    "test = OllamaLLM()\n",
    "print(test.complete(\"What model am I using?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aviation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
